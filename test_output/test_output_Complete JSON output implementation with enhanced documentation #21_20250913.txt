# PERFORMANCE TEST FAILURE 

Run # Run performance-specific tests
============================= test session starts ==============================
platform linux -- Python 3.11.13, pytest-8.4.2, pluggy-1.6.0 -- /opt/hostedtoolcache/Python/3.11.13/x64/bin/python
cachedir: .pytest_cache
rootdir: /home/runner/work/chatgpt-conversation-extractor/chatgpt-conversation-extractor
plugins: mock-3.15.0, cov-7.0.0
collecting ... collected 3 items / 1 deselected / 2 selected

tests/test_performance.py::TestPerformance::test_extraction_speed_small PASSED [ 50%]
tests/test_performance.py::TestPerformance::test_extraction_speed_large FAILED [100%]

=================================== FAILURES ===================================
_________________ TestPerformance.test_extraction_speed_large __________________
tests/test_performance.py:139: in test_extraction_speed_large
    assert len(md_files) == len(conversations)
E   AssertionError: assert 0 == 1000
E    +  where 0 = len([])
E    +  and   1000 = len([{'create_time': 1704067200, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0000', ...}, {'create_time': 1704067201, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0001', ...}, {'create_time': 1704067202, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0002', ...}, {'create_time': 1704067203, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0003', ...}, {'create_time': 1704067204, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0004', ...}, {'create_time': 1704067205, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0005', ...}, ...])
----------------------------- Captured stdout call -----------------------------

Processed 1000 conversations in 0.15s
Rate: 6492.7 conversations/second
=========================== short test summary info ============================
FAILED tests/test_performance.py::TestPerformance::test_extraction_speed_large - AssertionError: assert 0 == 1000
 +  where 0 = len([])
 +  and   1000 = len([{'create_time': 1704067200, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0000', ...}, {'create_time': 1704067201, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0001', ...}, {'create_time': 1704067202, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0002', ...}, {'create_time': 1704067203, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0003', ...}, {'create_time': 1704067204, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0004', ...}, {'create_time': 1704067205, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0005', ...}, ...])
================== 1 failed, 1 passed, 1 deselected in 0.31s ===================
Error: Process completed with exit code 1.


# UNIT TEST FAILURE


tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_extract_all_with_various_scenarios FAILED [  1%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_process_conversation_with_errors PASSED [  2%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_extract_metadata_comprehensive PASSED [  3%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_backward_traverse_edge_cases PASSED [  4%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_process_messages_comprehensive PASSED [  5%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_merge_continuations PASSED [  6%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_generate_markdown PASSED [  7%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_save_to_file_with_project FAILED [  8%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_sanitize_filename_various_inputs PASSED [  9%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_log_conversion_failure PASSED [ 10%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_save_conversion_log PASSED [ 11%]
tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_save_schema_report PASSED [ 12%]
tests/test_coverage_80_plus.py::TestMessageProcessorComprehensive::test_extract_message_content_all_types PASSED [ 13%]
tests/test_coverage_80_plus.py::TestMessageProcessorComprehensive::test_should_filter_message_various_cases PASSED [ 15%]
tests/test_coverage_80_plus.py::TestMessageProcessorComprehensive::test_extract_web_urls PASSED [ 16%]
tests/test_coverage_80_plus.py::TestMessageProcessorComprehensive::test_extract_file_names PASSED [ 17%]
tests/test_coverage_80_plus.py::TestTrackersComprehensive::test_schema_evolution_tracker PASSED [ 18%]
tests/test_coverage_80_plus.py::TestTrackersComprehensive::test_progress_tracker PASSED [ 19%]
tests/test_coverage_80_plus.py::TestIntegration::test_full_extraction_workflow FAILED [ 20%]
tests/test_coverage_final.py::TestCoverageFinal::test_extractor_print_summary PASSED [ 21%]
tests/test_coverage_final.py::TestCoverageFinal::test_extractor_error_paths PASSED [ 22%]
tests/test_coverage_final.py::TestCoverageFinal::test_message_processor_edge_cases PASSED [ 23%]
tests/test_coverage_final.py::TestCoverageFinal::test_tracker_milestones PASSED [ 24%]
tests/test_coverage_final.py::TestCoverageFinal::test_main_entry_point FAILED [ 25%]
tests/test_coverage_final.py::TestCoverageFinal::test_extractor_finding_leaf_nodes PASSED [ 26%]
tests/test_coverage_final.py::TestCoverageFinal::test_processor_multimodal_content PASSED [ 27%]
tests/test_coverage_final.py::TestCoverageFinal::test_schema_tracker_report_generation PASSED [ 29%]
tests/test_coverage_final.py::TestCoverageFinal::test_extractor_with_custom_instructions FAILED [ 30%]
tests/test_coverage_final.py::TestCoverageFinal::test_progress_tracker_final_stats PASSED [ 31%]
tests/test_coverage_final.py::TestCoverageFinal::test_extractor_metadata_extraction PASSED [ 32%]
tests/test_coverage_final.py::TestCoverageFinal::test_processor_citation_extraction PASSED [ 33%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_extractor_full_workflow FAILED [ 34%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_extractor_with_project_conversation FAILED [ 35%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_message_processor_content_types PASSED [ 36%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_message_filtering PASSED [ 37%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_schema_evolution_tracker PASSED [ 38%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_progress_tracker_operations PASSED [ 39%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_filename_sanitization PASSED [ 40%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_web_url_extraction PASSED [ 41%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_backward_traverse_complex PASSED [ 43%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_citation_extraction PASSED [ 44%]
tests/test_coverage_improvement.py::TestCoverageImprovement::test_file_attachment_extraction PASSED [ 45%]
tests/test_extractor.py::TestConversationExtractorV2::test_initialization PASSED [ 46%]
tests/test_extractor.py::TestConversationExtractorV2::test_extract_metadata PASSED [ 47%]
tests/test_extractor.py::TestConversationExtractorV2::test_backward_traverse PASSED [ 48%]
tests/test_extractor.py::TestConversationExtractorV2::test_backward_traverse_missing_current_node PASSED [ 49%]
tests/test_extractor.py::TestConversationExtractorV2::test_process_messages PASSED [ 50%]
tests/test_extractor.py::TestConversationExtractorV2::test_generate_markdown PASSED [ 51%]
tests/test_extractor.py::TestConversationExtractorV2::test_sanitize_filename PASSED [ 52%]
tests/test_extractor.py::TestConversationExtractorV2::test_save_to_file FAILED [ 53%]
tests/test_extractor.py::TestConversationExtractorV2::test_save_to_file_with_project FAILED [ 54%]
tests/test_extractor.py::TestConversationExtractorV2::test_extract_all_integration FAILED [ 55%]
tests/test_main.py::TestCLI::test_version_argument FAILED                [ 56%]
tests/test_main.py::TestCLI::test_help_argument PASSED                   [ 58%]
tests/test_main.py::TestCLI::test_missing_input_file FAILED              [ 59%]
tests/test_main.py::TestCLI::test_successful_extraction FAILED           [ 60%]
tests/test_main.py::TestCLI::test_run_failure_analysis_no_log PASSED     [ 61%]
tests/test_main.py::TestCLI::test_run_failure_analysis_no_failures PASSED [ 62%]
tests/test_performance.py::TestPerformance::test_extraction_speed_small PASSED [ 63%]
tests/test_performance.py::TestPerformance::test_extraction_speed_large FAILED [ 64%]
tests/test_performance.py::TestPerformance::test_memory_usage PASSED     [ 65%]
tests/test_processors.py::TestMessageProcessor::test_should_filter_message_visually_hidden PASSED [ 66%]
tests/test_processors.py::TestMessageProcessor::test_should_filter_message_system_non_user PASSED [ 67%]
tests/test_processors.py::TestMessageProcessor::test_should_not_filter_user_system_message PASSED [ 68%]
tests/test_processors.py::TestMessageProcessor::test_should_filter_tool_without_dalle PASSED [ 69%]
tests/test_processors.py::TestMessageProcessor::test_should_not_filter_tool_with_dalle PASSED [ 70%]
tests/test_processors.py::TestMessageProcessor::test_extract_message_content_text PASSED [ 72%]
tests/test_processors.py::TestMessageProcessor::test_extract_message_content_code PASSED [ 73%]
tests/test_processors.py::TestMessageProcessor::test_extract_message_content_execution_output PASSED [ 74%]
tests/test_processors.py::TestMessageProcessor::test_extract_message_content_multimodal PASSED [ 75%]
tests/test_processors.py::TestMessageProcessor::test_extract_from_parts_with_none PASSED [ 76%]
tests/test_processors.py::TestMessageProcessor::test_extract_citations PASSED [ 77%]
tests/test_processors.py::TestMessageProcessor::test_extract_web_urls_from_citations PASSED [ 78%]
tests/test_processors.py::TestMessageProcessor::test_extract_web_urls_from_content PASSED [ 79%]
tests/test_processors.py::TestMessageProcessor::test_extract_web_urls_from_text PASSED [ 80%]
tests/test_processors.py::TestMessageProcessor::test_extract_file_names_from_attachments PASSED [ 81%]
tests/test_processors.py::TestMessageProcessor::test_contains_dalle_image_none_metadata PASSED [ 82%]
tests/test_processors.py::TestMessageProcessor::test_user_editable_context_extraction PASSED [ 83%]
tests/test_trackers.py::TestSchemaEvolutionTracker::test_track_known_content_type PASSED [ 84%]
tests/test_trackers.py::TestSchemaEvolutionTracker::test_track_unknown_content_type PASSED [ 86%]
tests/test_trackers.py::TestSchemaEvolutionTracker::test_track_author_roles PASSED [ 87%]
tests/test_trackers.py::TestSchemaEvolutionTracker::test_track_metadata_keys PASSED [ 88%]
tests/test_trackers.py::TestSchemaEvolutionTracker::test_track_part_types PASSED [ 89%]
tests/test_trackers.py::TestSchemaEvolutionTracker::test_sample_limit PASSED [ 90%]
tests/test_trackers.py::TestSchemaEvolutionTracker::test_generate_report PASSED [ 91%]
tests/test_trackers.py::TestProgressTracker::test_initialization PASSED  [ 92%]
tests/test_trackers.py::TestProgressTracker::test_update_success PASSED  [ 93%]
tests/test_trackers.py::TestProgressTracker::test_update_failure PASSED  [ 94%]
tests/test_trackers.py::TestProgressTracker::test_final_stats PASSED     [ 95%]
tests/test_trackers.py::TestProgressTracker::test_show_progress_output PASSED [ 96%]
tests/test_trackers.py::TestProgressTracker::test_eta_calculation PASSED [ 97%]
tests/test_trackers.py::TestProgressTracker::test_milestone_updates PASSED [ 98%]
tests/test_trackers.py::TestProgressTracker::test_zero_total_handling PASSED [100%]
ERROR: Coverage failure: total of 66 is less than fail-under=70


# FAILURES IN UNIT TESTS - OUTPUT

================================== FAILURES ===================================
_____ TestExtractorComprehensive.test_extract_all_with_various_scenarios ______

self = <tests.test_coverage_80_plus.TestExtractorComprehensive object at 0x000002C99F4B2940>
tmp_path = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extract_all_with_various_0')

    def test_extract_all_with_various_scenarios(self, tmp_path):
        """Test extract_all with different conversation scenarios."""
        conversations = [
            # Normal conversation
            {
                "id": "conv-1",
                "title": "Normal Chat",
                "create_time": 1234567890,
                "update_time": 1234567900,
                "mapping": {
                    "n1": {"id": "n1", "parent": None, "children": ["n2"]},
                    "n2": {"id": "n2", "parent": "n1", "children": [], "message": {
                        "author": {"role": "user"},
                        "content": {"content_type": "text", "parts": ["Hello"]}
                    }}
                },
                "current_node": "n2"
            },
            # Conversation with missing current_node
            {
                "id": "conv-2",
                "title": "Missing Current",
                "mapping": {
                    "n1": {"id": "n1", "parent": None, "children": ["n2"]},
                    "n2": {"id": "n2", "parent": "n1", "children": [], "message": {
                        "author": {"role": "user"},
                        "content": {"content_type": "text", "parts": ["Test"]}
                    }}
                }
            },
            # Empty mapping
            {
                "id": "conv-3",
                "title": "Empty Mapping",
                "mapping": {},
                "current_node": "n1"
            }
        ]
    
        input_file = tmp_path / "test.json"
        input_file.write_text(json.dumps(conversations))
        output_dir = tmp_path / "output"
    
        extractor = ConversationExtractorV2(str(input_file), str(output_dir))
        extractor.extract_all()
    
        # Check files created
        assert output_dir.exists()
>       assert (output_dir / "Normal Chat.md").exists()
E       AssertionError: assert False
E        +  where False = exists()
E        +    where exists = (WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extract_all_with_various_0/output') / 'Normal Chat.md').exists

tests\test_coverage_80_plus.py:70: AssertionError
---------------------------- Captured stdout call -----------------------------

  Progress: 3/3 (100.0%) | Failed: 0 | Rate: 746.0/s | ETA: 0s
__________ TestExtractorComprehensive.test_save_to_file_with_project __________

self = <tests.test_coverage_80_plus.TestExtractorComprehensive object at 0x000002C99F6369A0>
tmp_path = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_save_to_file_with_project0')

    def test_save_to_file_with_project(self, tmp_path):
        """Test saving file with project organization."""
        extractor = ConversationExtractorV2("dummy.json", str(tmp_path))
    
        metadata = {
            "id": "test-123",
            "title": "Project File",
            "project_id": "g-p-myproject"
        }
    
        content = "# Test Content"
>       extractor.save_to_file(metadata, content)
E       AttributeError: 'ConversationExtractorV2' object has no attribute 'save_to_file'

tests\test_coverage_80_plus.py:224: AttributeError
________________ TestIntegration.test_full_extraction_workflow ________________

self = <tests.test_coverage_80_plus.TestIntegration object at 0x000002C99F647370>
tmp_path = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_full_extraction_workflow0')

    def test_full_extraction_workflow(self, tmp_path):
        """Test complete extraction workflow."""
        # Create comprehensive test data
        conversations = [
            {
                "id": "conv-complete",
                "title": "Complete Test",
                "create_time": 1234567890,
                "update_time": 1234567900,
                "conversation_template_id": "g-p-test-project",
                "custom_instructions": {
                    "about_user": "Test user",
                    "about_model": "Test model"
                },
                "mapping": {
                    "root": {"id": "root", "parent": None, "children": ["msg1"]},
                    "msg1": {"id": "msg1", "parent": "root", "children": ["msg2"],
                            "message": {
                                "author": {"role": "user"},
                                "content": {"content_type": "text", "parts": ["Hello AI"]},
                                "metadata": {}
                            }},
                    "msg2": {"id": "msg2", "parent": "msg1", "children": ["msg3"],
                            "message": {
                                "author": {"role": "assistant"},
                                "content": {"content_type": "text", "parts": ["Hello human!"]},
                                "metadata": {}
                            }},
                    "msg3": {"id": "msg3", "parent": "msg2", "children": [],
                            "message": {
                                "author": {"role": "user"},
                                "content": {
                                    "content_type": "code",
                                    "language": "python",
                                    "text": "def greet():\n    print('Hi')"
                                },
                                "metadata": {"attachments": [{"name": "script.py"}]}
                            }}
                },
                "current_node": "msg3"
            }
        ]
    
        input_file = tmp_path / "complete.json"
        input_file.write_text(json.dumps(conversations))
        output_dir = tmp_path / "output"
    
        # Run extraction
        extractor = ConversationExtractorV2(str(input_file), str(output_dir))
        extractor.extract_all()
    
        # Verify results
        project_dir = output_dir / "g-p-test-project"
>       assert project_dir.exists()
E       AssertionError: assert False
E        +  where False = exists()
E        +    where exists = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_full_extraction_workflow0/output/g-p-test-project').exists

tests\test_coverage_80_plus.py:513: AssertionError
---------------------------- Captured stdout call -----------------------------

  Progress: 1/1 (100.0%) | Failed: 0 | Rate: 248.4/s | ETA: 0s
___________________ TestCoverageFinal.test_main_entry_point ___________________

self = <tests.test_coverage_final.TestCoverageFinal object at 0x000002C99F63B550>
tmp_path = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_main_entry_point0')

    def test_main_entry_point(self, tmp_path):
        """Test __main__ module entry point."""
        from chatgpt_extractor import __main__ as main_module
    
        # Create test data
        conversations = [{"id": "test", "title": "Test", "mapping": {}, "current_node": "n1"}]
        input_file = tmp_path / "test.json"
        input_file.write_text(json.dumps(conversations))
    
        # Test with command line args
        test_args = ["extract.py", str(input_file), str(tmp_path / "output")]
    
        with patch('sys.argv', test_args):
            # This tests the main entry point
            try:
>               main_module.main()

tests\test_coverage_final.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
src\chatgpt_extractor\__main__.py:148: in main
    validate_cli_arguments(args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = Namespace(input_file='C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pytest-of-runneradmin\\pytest-0\\test_main_entry_p...iple', markdown_dir=None, json_dir=None, json_file=None, preserve_timestamps=True, analyze_failures=False, debug=False)

    def validate_cli_arguments(args: argparse.Namespace) -> None:
        """Validate CLI argument combinations for consistency.
    
        Ensures mutually exclusive options aren't used together and that
        format-specific arguments are only used with appropriate output formats.
        JSON-specific validations prevent configuration conflicts early.
        """
        logger = get_logger(__name__)
    
        # Prevent conflicting JSON output specifications that would create ambiguous behavior
        if args.json_dir and args.json_file:
            logger.error("Cannot specify both --json-dir and --json-file")
            sys.exit(1)
    
        # Ensure format selection aligns with output type to prevent configuration errors
        if args.json_format and args.output_format == 'markdown':
            logger.error("--json-format can only be used when --output-format includes 'json' or 'both'")
>           sys.exit(1)
E           SystemExit: 1

src\chatgpt_extractor\__main__.py:59: SystemExit

During handling of the above exception, another exception occurred:

self = <tests.test_coverage_final.TestCoverageFinal object at 0x000002C99F63B550>
tmp_path = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_main_entry_point0')

    def test_main_entry_point(self, tmp_path):
        """Test __main__ module entry point."""
        from chatgpt_extractor import __main__ as main_module
    
        # Create test data
        conversations = [{"id": "test", "title": "Test", "mapping": {}, "current_node": "n1"}]
        input_file = tmp_path / "test.json"
        input_file.write_text(json.dumps(conversations))
    
        # Test with command line args
        test_args = ["extract.py", str(input_file), str(tmp_path / "output")]
    
        with patch('sys.argv', test_args):
            # This tests the main entry point
            try:
                main_module.main()
            except SystemExit as e:
                # Check exit code is 0 (success)
>               assert e.code == 0
E               assert 1 == 0
E                +  where 1 = SystemExit(1).code

tests\test_coverage_final.py:93: AssertionError
---------------------------- Captured stdout call -----------------------------
[2025-09-13 21:28:59.339] [ERROR   ] [chatgpt_extractor.chatgpt_extractor.__main__:validate_cli_arguments:58] - --json-format can only be used when --output-format includes 'json' or 'both'
------------------------------ Captured log call ------------------------------
INFO     chatgpt_extractor:logging_config.py:226 Logging configured - Level: INFO, JSON: False, tqdm: False, File logging: True
ERROR    chatgpt_extractor.chatgpt_extractor.__main__:__main__.py:58 --json-format can only be used when --output-format includes 'json' or 'both'
__________ TestCoverageFinal.test_extractor_with_custom_instructions __________

self = <tests.test_coverage_final.TestCoverageFinal object at 0x000002C99F650F10>
tmp_path = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extractor_with_custom_ins0')

    def test_extractor_with_custom_instructions(self, tmp_path):
        """Test extraction with custom instructions."""
        conv = {
            "id": "custom-test",
            "title": "Custom Instructions Test",
            "custom_instructions": {
                "enabled": True,
                "about_user": "User context here",
                "about_model": "Assistant context here"
            },
            "mapping": {
                "n1": {"id": "n1", "parent": None, "children": [],
                      "message": {"author": {"role": "user"},
                                "content": {"content_type": "text", "parts": ["Test"]}}}
            },
            "current_node": "n1"
        }
    
        input_file = tmp_path / "custom.json"
        input_file.write_text(json.dumps([conv]))
    
        extractor = ConversationExtractorV2(str(input_file), str(tmp_path / "output"))
        extractor.extract_all()
    
        # Check file was created
>       assert (tmp_path / "output" / "Custom Instructions Test.md").exists()
E       AssertionError: assert False
E        +  where False = exists()
E        +    where exists = ((WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extractor_with_custom_ins0') / 'output') / 'Custom Instructions Test.md').exists

tests\test_coverage_final.py:177: AssertionError
---------------------------- Captured stdout call -----------------------------

  Progress: 1/1 (100.0%) | Failed: 0 | Rate: 497.5/s | ETA: 0s
------------------------------ Captured log call ------------------------------
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:169 ChatGPT Conversation Extractor v2.0
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:170 ============================================================
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:173 Loading conversations from C:\Users\runneradmin\AppData\Local\Temp\pytest-of-runneradmin\pytest-0\test_extractor_with_custom_ins0\custom.json
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:190 Found 1 conversations to process
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:191 Output directory: C:\Users\runneradmin\AppData\Local\Temp\pytest-of-runneradmin\pytest-0\test_extractor_with_custom_ins0\output
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:193 Markdown output: C:\Users\runneradmin\AppData\Local\Temp\pytest-of-runneradmin\pytest-0\test_extractor_with_custom_ins0\output\md
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1076 
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1077 ============================================================
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1078 EXTRACTION COMPLETE!
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1079 ============================================================
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1080   Total conversations: 1
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1081   Successfully processed: 1
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1082   Failed: 0
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1083   Success rate: 100.0%
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1087   Markdown files created: 1
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1093   Time elapsed: 0.0s
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1094   Processing rate: 357.8 conv/s
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1095   Output directory: C:\Users\runneradmin\AppData\Local\Temp\pytest-of-runneradmin\pytest-0\test_extractor_with_custom_ins0\output
INFO     chatgpt_extractor.chatgpt_extractor.extractor:extractor.py:1096 ============================================================
____________ TestCoverageImprovement.test_extractor_full_workflow _____________

self = <tests.test_coverage_improvement.TestCoverageImprovement object at 0x000002C99F516E80>
tmp_path = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extractor_full_workflow0')

    def test_extractor_full_workflow(self, tmp_path):
        """End-to-end extraction with nested nodes and code content type validation."""
        conversations = [{
            "id": "test-123",
            "title": "Test Conversation",
            "create_time": 1234567890,
            "update_time": 1234567900,
            "mapping": {
                "n1": {"id": "n1", "parent": None, "children": ["n2"], "message": None},
                "n2": {"id": "n2", "parent": "n1", "children": ["n3"], "message": {
                    "author": {"role": "user"},
                    "content": {"content_type": "text", "parts": ["Hello"]}
                }},
                "n3": {"id": "n3", "parent": "n2", "children": ["n4"], "message": {
                    "author": {"role": "assistant"},
                    "content": {"content_type": "text", "parts": ["Hi there!"]}
                }},
                "n4": {"id": "n4", "parent": "n3", "children": [], "message": {
                    "author": {"role": "user"},
                    "content": {"content_type": "code", "language": "python", "text": "print('test')"}
                }}
            },
            "current_node": "n4"
        }]
    
        input_file = tmp_path / "test.json"
        input_file.write_text(json.dumps(conversations))
        output_dir = tmp_path / "output"
    
        extractor = ConversationExtractorV2(str(input_file), str(output_dir))
        extractor.extract_all()
    
>       assert (output_dir / "Test Conversation.md").exists()
E       AssertionError: assert False
E        +  where False = exists()
E        +    where exists = (WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extractor_full_workflow0/output') / 'Test Conversation.md').exists

tests\test_coverage_improvement.py:57: AssertionError
src\chatgpt_extractor\__main__.py            78     39    50%   28-39, 53-54, 62-69, 150-186, 190
src\chatgpt_extractor\extractor.py          657    266    60%   110, 117-132, 153-154, 158-165, 179-188, 195-198, 223-231, 234-238, 242-246, 253, 297, 302, 371, 374-377, 386-419, 472, 482, 485-486, 536, 540, 548, 553-558, 596-598, 601-603, 613, 615, 633-637, 659, 668-678, 681-684, 714-719, 726-736, 743-751, 788-826, 843-867, 883-914, 947-948, 952-954, 969, 984-991, 1007-1029, 1033-1034, 1047-1070, 1089, 1091, 1107-1108, 1119, 1121, 1123, 1125, 1138-1159, 1162, 1164, 1166, 1170-1171, 1178-1179, 1203, 1242, 1245-1261
src\chatgpt_extractor\logging_config.py     128     51    60%   41, 53-58, 70-82, 89-117, 150, 155, 162, 177, 182-184, 196-197, 209-210, 222-223, 257-263, 301-309
src\chatgpt_extractor\processors.py         240     65    73%   49, 55, 79, 142-145, 150-152, 155-167, 170-176, 179-181, 189, 191, 199, 201, 203, 233-239, 243-245, 249, 253, 257, 261-263, 325-333, 337-340, 363, 390-395
src\chatgpt_extractor\trackers.py           148      3    98%   13, 197, 203
-----------------------------------------------------------------------
TOTAL                                      1257    424    66%
Coverage XML written to file coverage.xml
FAIL Required test coverage of 70% not reached. Total coverage: 66.27%
=========================== short test summary info ===========================
FAILED tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_extract_all_with_various_scenarios - AssertionError: assert False
 +  where False = exists()
 +    where exists = (WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extract_all_with_various_0/output') / 'Normal Chat.md').exists
FAILED tests/test_coverage_80_plus.py::TestExtractorComprehensive::test_save_to_file_with_project - AttributeError: 'ConversationExtractorV2' object has no attribute 'save_to_file'
FAILED tests/test_coverage_80_plus.py::TestIntegration::test_full_extraction_workflow - AssertionError: assert False
 +  where False = exists()
 +    where exists = WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_full_extraction_workflow0/output/g-p-test-project').exists
FAILED tests/test_coverage_final.py::TestCoverageFinal::test_main_entry_point - assert 1 == 0
 +  where 1 = SystemExit(1).code
FAILED tests/test_coverage_final.py::TestCoverageFinal::test_extractor_with_custom_instructions - AssertionError: assert False
 +  where False = exists()
 +    where exists = ((WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extractor_with_custom_ins0') / 'output') / 'Custom Instructions Test.md').exists
FAILED tests/test_coverage_improvement.py::TestCoverageImprovement::test_extractor_full_workflow - AssertionError: assert False
 +  where False = exists()
 +    where exists = (WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extractor_full_workflow0/output') / 'Test Conversation.md').exists
FAILED tests/test_coverage_improvement.py::TestCoverageImprovement::test_extractor_with_project_conversation - AssertionError: assert False
 +  where False = exists()
 +    where exists = ((WindowsPath('C:/Users/runneradmin/AppData/Local/Temp/pytest-of-runneradmin/pytest-0/test_extractor_with_project_co0/output') / 'g-p-myproject') / 'Project Chat.md').exists
FAILED tests/test_extractor.py::TestConversationExtractorV2::test_save_to_file - AttributeError: 'ConversationExtractorV2' object has no attribute 'save_to_file'
FAILED tests/test_extractor.py::TestConversationExtractorV2::test_save_to_file_with_project - AttributeError: 'ConversationExtractorV2' object has no attribute 'save_to_file'
FAILED tests/test_extractor.py::TestConversationExtractorV2::test_extract_all_integration - assert 0 == 1
 +  where 0 = len([])
FAILED tests/test_main.py::TestCLI::test_version_argument - AssertionError: assert 'v2.0' in 'ChatGPT Conversation Extractor v3.1\n'
 +  where 'ChatGPT Conversation Extractor v3.1\n' = CaptureResult(out='ChatGPT Conversation Extractor v3.1\n', err='').out
FAILED tests/test_main.py::TestCLI::test_missing_input_file - assert 'not found' in "[2025-09-13 21:28:59.535] [ERROR   ] [chatgpt_extractor.chatgpt_extractor.__main__:validate_cli_arguments:58] - --json-format can only be used when --output-format includes 'json' or 'both'\n"
 +  where "[2025-09-13 21:28:59.535] [ERROR   ] [chatgpt_extractor.chatgpt_extractor.__main__:validate_cli_arguments:58] - --json-format can only be used when --output-format includes 'json' or 'both'\n" = CaptureResult(out="[2025-09-13 21:28:59.535] [ERROR   ] [chatgpt_extractor.chatgpt_extractor.__main__:validate_cli_arguments:58] - --json-format can only be used when --output-format includes 'json' or 'both'\n", err='').out
FAILED tests/test_main.py::TestCLI::test_successful_extraction - SystemExit: 1
FAILED tests/test_performance.py::TestPerformance::test_extraction_speed_large - AssertionError: assert 0 == 1000
 +  where 0 = len([])
 +  and   1000 = len([{'create_time': 1704067200, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0000', ...}, {'create_time': 1704067201, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0001', ...}, {'create_time': 1704067202, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0002', ...}, {'create_time': 1704067203, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0003', ...}, {'create_time': 1704067204, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0004', ...}, {'create_time': 1704067205, 'current_node': 'node-3', 'default_model_slug': 'gpt-4', 'id': 'perf-test-0005', ...}, ...])
======================== 14 failed, 79 passed in 5.77s ========================